# rag_chat.py
import os
import shutil
import argparse # argparse ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Document, # Document ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from dotenv import load_dotenv

# --- å®šæ•°å®šç¾© ---
PERSIST_DIR = "./storage"
VAULT_PATH = "test-vault"
EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-4o-mini"

# load_or_create_index é–¢æ•°ã¯å¤‰æ›´ãªã— (å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã¾ã¾ã§OK)
def load_or_create_index(persist_dir: str, vault_path: str, embed_model: OpenAIEmbedding, force_recreate: bool = False) -> VectorStoreIndex | None:
    """
    æ°¸ç¶šåŒ–ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€ã‹ã€ãªã‘ã‚Œã°æ–°ã—ãä½œæˆã™ã‚‹ã€‚
    force_recreate=True ã®å ´åˆã€ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãŸå ´åˆã«å†ä½œæˆã™ã‚‹ã€‚
    """
    index = None
    should_create_index = False

    if force_recreate and os.path.exists(persist_dir):
        print(f"å¼·åˆ¶å†ä½œæˆãƒ•ãƒ©ã‚°ãŒæŒ‡å®šã•ã‚ŒãŸãŸã‚ã€å¤ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{persist_dir}' ã‚’å‰Šé™¤ã—ã¦ã„ã¾ã™...")
        try:
            shutil.rmtree(persist_dir)
            print(f"'{persist_dir}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
            should_create_index = True
        except OSError as e:
            print(f"ã‚¨ãƒ©ãƒ¼: '{persist_dir}' ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            should_create_index = True

    if not should_create_index and os.path.exists(persist_dir):
        while True:
            reload_choice = input(f"ğŸ’¾ ä¿å­˜ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ (Y/n): ").strip().lower()
            if reload_choice == 'n':
                print(f"å¤ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{persist_dir}' ã‚’å‰Šé™¤ã—ã¦ã„ã¾ã™...")
                try:
                    shutil.rmtree(persist_dir)
                    print(f"'{persist_dir}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                    should_create_index = True
                except OSError as e:
                    print(f"ã‚¨ãƒ©ãƒ¼: '{persist_dir}' ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    should_create_index = True
                break
            elif reload_choice == 'y' or reload_choice == '':
                try:
                    print(f"æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ '{persist_dir}' ã‹ã‚‰èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
                    # LlamaIndex v0.10ä»¥é™ã§ã¯ã€loadæ™‚ã«embed_modelã‚’ç›´æ¥æ¸¡ã•ãªã„ã“ã¨ãŒå¤šã„
                    # StorageContextçµŒç”±ã§ãƒ­ãƒ¼ãƒ‰ã—ã€å¾Œã§QueryEngineä½œæˆæ™‚ã«LLMã‚„Embed Modelã‚’è¨­å®šã™ã‚‹
                    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
                    # Settings ã‚’ä½¿ã£ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«è¨­å®šã™ã‚‹ã‹ã€ãƒ­ãƒ¼ãƒ‰å¾Œã«è¨­å®šã™ã‚‹
                    # ã“ã“ã§ã¯ãƒ­ãƒ¼ãƒ‰å¾Œã« index ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰è¨­å®šã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯é›£ã—ã„ã®ã§ã€
                    # QueryEngine ä½œæˆæ™‚ã« embed_model ã‚’æ„è­˜ã•ã›ã‚‹
                    index = load_index_from_storage(storage_context)
                    print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                except Exception as e:
                    print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†ä½œæˆã—ã¾ã™ã€‚")
                    should_create_index = True
                break
            else:
                print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚'y' ã¾ãŸã¯ 'n' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        if not should_create_index:
             print(f"'{persist_dir}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        should_create_index = True

    if should_create_index:
        try:
            print(f"'{vault_path}' ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã™...")
            documents = SimpleDirectoryReader(vault_path).load_data()
            if not documents:
                print(f"è­¦å‘Š: '{vault_path}' å†…ã«èª­ã¿è¾¼ã‚ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒãªãã¦ã‚‚ç©ºã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ä½œæˆã§ãã‚‹å ´åˆãŒã‚ã‚‹ãŒã€
                # RAGã®æ„å‘³ãŒãªããªã‚‹ãŸã‚ã€ã“ã“ã§ã¯Noneã‚’è¿”ã™æ–¹ãŒé©åˆ‡ã‹ã‚‚ã—ã‚Œãªã„
                # return None
                print("ç©ºã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚") # ã¾ãŸã¯ç©ºã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
                index = VectorStoreIndex.from_documents([], embed_model=embed_model)

            else:
                 # Settings ã‚’ä½¿ã£ã¦ embed_model ã‚’è¨­å®šã™ã‚‹ä¾‹
                 # Settings.embed_model = embed_model
                 index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)

            print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ '{persist_dir}' ã«ä¿å­˜ã—ã¦ã„ã¾ã™...")
            os.makedirs(persist_dir, exist_ok=True)
            index.storage_context.persist(persist_dir=persist_dir)
            print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ '{persist_dir}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆã¾ãŸã¯ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None

    return index


# run_chat_loop é–¢æ•°ã¯å¤‰æ›´ãªã—
def run_chat_loop(query_engine):
    """ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã™ã‚‹"""
    print("\n--- ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹ã—ã¾ã™ ---")
    while True:
        try:
            q = input("\nğŸ§  è³ªå•ã—ã¦ãã ã•ã„ï¼š")
            if q.strip().lower() in ["exit", "quit"]:
                print("ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            if not q.strip():
                continue

            print("æ€è€ƒä¸­...")
            response = query_engine.query(q)
            print(f"\nğŸ—£ï¸ å›ç­”:\n{response}")

        except EOFError:
             print("\nãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
             break
        except KeyboardInterrupt:
            print("\nãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        except Exception as e:
            print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description="Markdown Vault ã«åŸºã¥ããƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
    parser.add_argument(
        "--force-recreate",
        action="store_true",
        help="æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç„¡è¦–ã—ã€å¼·åˆ¶çš„ã«å†ä½œæˆã—ã¾ã™ã€‚"
    )
    parser.add_argument(
        "--no-rag",
        action="store_true",
        help="Vault ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¾ãšã€LLM ã®ã¿ã®å¿œç­”ã«ã—ã¾ã™ã€‚"
    )
    args = parser.parse_args()

    load_dotenv()
    if not os.getenv("OPENAI_API_KEY"):
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'OPENAI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print(".env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€'OPENAI_API_KEY=YOUR_API_KEY' ã®å½¢å¼ã§ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return

    try:
        llm = OpenAI(model=LLM_MODEL, temperature=0.2)
        embed_model = OpenAIEmbedding(model=EMBEDDING_MODEL)
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return

    index = None
    if args.no_rag:
        print("--- RAGæ©Ÿèƒ½ ç„¡åŠ¹ãƒ¢ãƒ¼ãƒ‰ ---")
        try:
            # ç©ºãƒªã‚¹ãƒˆã§ã¯ãªãã€ãƒ€ãƒŸãƒ¼ã®Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’1ã¤æ¸¡ã™
            dummy_document = Document(text="This is a dummy document to initialize the index.")
            index = VectorStoreIndex.from_documents([dummy_document], embed_model=embed_model)
            print("ãƒ€ãƒŸãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸã€‚LLM ã®ã¿ã®å¿œç­”ã«ãªã‚Šã¾ã™ã€‚")
        except Exception as e:
             print(f"ãƒ€ãƒŸãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
             return

    else:
        print("--- RAGæ©Ÿèƒ½ æœ‰åŠ¹ãƒ¢ãƒ¼ãƒ‰ ---")
        index = load_or_create_index(PERSIST_DIR, VAULT_PATH, embed_model, force_recreate=args.force_recreate)

    if index is None:
        print("ã‚¨ãƒ©ãƒ¼: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    query_engine = index.as_query_engine(llm=llm)
    run_chat_loop(query_engine)

if __name__ == "__main__":
    main()