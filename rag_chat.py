# rag_chat.py

from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI 
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ã«OPENAI_API_KEY=xxx ã‚’å…¥ã‚Œã‚‹ï¼‰
load_dotenv()

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
path_to_vault = "test-vault"
documents = SimpleDirectoryReader(path_to_vault).load_data()

# åŸ‹ã‚è¾¼ã¿ã¨ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
embed_model = OpenAIEmbedding(model="text-embedding-3-small")
llm = OpenAI(model="gpt-4o-mini", temperature=0.2)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆåˆå›ã®ã¿å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰
index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)

# ãƒãƒ£ãƒƒãƒˆé–‹å§‹ï¼
query_engine = index.as_query_engine(llm=llm)

while True:
    q = input("\nğŸ§  è³ªå•ã—ã¦ãã ã•ã„ï¼š")
    if q.strip().lower() in ["exit", "quit"]:
        break
    response = query_engine.query(q)
    print(f"\nğŸ—£ï¸ å›ç­”:\n{response}")
