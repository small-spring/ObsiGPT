# rag_chat.py
import os
import shutil
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from dotenv import load_dotenv

# --- å®šæ•°å®šç¾© ---
PERSIST_DIR = "./storage"
VAULT_PATH = "test-vault"
EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-4o-mini"

def load_or_create_index(persist_dir: str, vault_path: str, embed_model: OpenAIEmbedding) -> VectorStoreIndex | None:
    """
    æ°¸ç¶šåŒ–ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€ã‹ã€ãªã‘ã‚Œã°æ–°ã—ãä½œæˆã™ã‚‹ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å†ä½œæˆã®é¸æŠè‚¢ã‚’æç¤ºã™ã‚‹ã€‚
    """
    index = None
    should_create_index = False

    if os.path.exists(persist_dir):
        while True:
            reload_choice = input(f"ğŸ’¾ ä¿å­˜ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ (Y/n): ").strip().lower()
            if reload_choice == 'n':
                print(f"å¤ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{persist_dir}' ã‚’å‰Šé™¤ã—ã¦ã„ã¾ã™...")
                try:
                    shutil.rmtree(persist_dir)
                    print(f"'{persist_dir}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                    should_create_index = True
                except OSError as e:
                    print(f"ã‚¨ãƒ©ãƒ¼: '{persist_dir}' ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    # å‰Šé™¤ã«å¤±æ•—ã—ãŸå ´åˆã§ã‚‚ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã«é€²ã‚€
                    should_create_index = True
                break
            elif reload_choice == 'y' or reload_choice == '':
                try:
                    print(f"æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ '{persist_dir}' ã‹ã‚‰èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
                    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
                    index = load_index_from_storage(storage_context, embed_model=embed_model)
                    print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                except Exception as e:
                    print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†ä½œæˆã—ã¾ã™ã€‚")
                    should_create_index = True
                break
            else:
                print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚'y' ã¾ãŸã¯ 'n' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        print(f"'{persist_dir}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        should_create_index = True

    if should_create_index:
        try:
            print(f"'{vault_path}' ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã™...")
            documents = SimpleDirectoryReader(vault_path).load_data()
            if not documents:
                print(f"è­¦å‘Š: '{vault_path}' å†…ã«èª­ã¿è¾¼ã‚ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return None # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒãªã„å ´åˆã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã§ããªã„
            index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
            print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ '{persist_dir}' ã«ä¿å­˜ã—ã¦ã„ã¾ã™...")
            index.storage_context.persist(persist_dir=persist_dir)
            print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ '{persist_dir}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆã¾ãŸã¯ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯ None ã‚’è¿”ã™

    return index

def run_chat_loop(query_engine):
    """ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã™ã‚‹"""
    print("\n--- ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹ã—ã¾ã™ ---")
    while True:
        try:
            q = input("\nğŸ§  è³ªå•ã—ã¦ãã ã•ã„ï¼š")
            if q.strip().lower() in ["exit", "quit"]:
                print("ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            if not q.strip():
                continue # ç©ºã®å…¥åŠ›ã¯ç„¡è¦–

            print("æ€è€ƒä¸­...") # å¿œç­”å¾…æ©Ÿä¸­ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            response = query_engine.query(q)
            print(f"\nğŸ—£ï¸ å›ç­”:\n{response}")

        except EOFError: # Ctrl+D ãªã©ã§çµ‚äº†ã—ãŸå ´åˆ
             print("\nãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
             break
        except KeyboardInterrupt: # Ctrl+C ã§çµ‚äº†ã—ãŸå ´åˆ
            print("\nãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        except Exception as e:
            print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ«ãƒ¼ãƒ—ã‚’ç¶™ç¶šã™ã‚‹å ´åˆãŒå¤šã„ãŒã€å¿…è¦ã«å¿œã˜ã¦ break ã™ã‚‹
            # break


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
    load_dotenv()
    if not os.getenv("OPENAI_API_KEY"):
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'OPENAI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print(".env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€'OPENAI_API_KEY=YOUR_API_KEY' ã®å½¢å¼ã§ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return # APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯çµ‚äº†

    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    try:
        embed_model = OpenAIEmbedding(model=EMBEDDING_MODEL)
        llm = OpenAI(model=LLM_MODEL, temperature=0.2)
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿/ä½œæˆ
    index = load_or_create_index(PERSIST_DIR, VAULT_PATH, embed_model)

    if index is None:
        print("ã‚¨ãƒ©ãƒ¼: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã®ä½œæˆ
    query_engine = index.as_query_engine(llm=llm)

    # ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ
    run_chat_loop(query_engine)

if __name__ == "__main__":
    main()